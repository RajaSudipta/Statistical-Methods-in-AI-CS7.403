{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import comb, pow, log2\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Binomial\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "metadata": {
        "id": "-j1zzDT3D7cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next():\n",
        "    global index_permutation\n",
        "    global cur_ind\n",
        "    global data_batch_size\n",
        "    global data\n",
        "    \n",
        "    if cur_ind < num_data:\n",
        "        indices = index_permutation[cur_ind : cur_ind + data_batch_size]\n",
        "        cur_ind += data_batch_size\n",
        "        return data[indices]\n",
        "\n",
        "    cur_ind = 0\n",
        "    return None\n",
        "\n",
        "\n",
        "def calculate_beta_tilde_t(beta_tilde_t, cur_t):\n",
        "    a = beta_tilde_t[cur_t-1] + beta_t(cur_t)\n",
        "    b = beta_t(cur_t) * beta_tilde_t[cur_t-1]\n",
        "    return (a - b)\n",
        "\n",
        "def validate(model, num_samples, batch_size):\n",
        "    train_data = []\n",
        "    with open(os.path.join('train.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            train_data.append(torch.FloatTensor([float(x) for x in line.strip()]).to(\"cpu\"))\n",
        "        train_data = torch.vstack(train_data)\n",
        "\n",
        "    train_data = train_data.cpu().detach().numpy().tolist()\n",
        "\n",
        "    val_data = []\n",
        "    with open(os.path.join('val.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            val_data.append(torch.FloatTensor([float(x) for x in line.strip()]).to(\"cpu\"))\n",
        "        val_data = torch.vstack(val_data)\n",
        "\n",
        "    val_data = val_data.cpu().detach().numpy().tolist()\n",
        "\n",
        "    train_count = 0\n",
        "    val_count = 0\n",
        "    other_count = 0\n",
        "\n",
        "    sample_count = 0\n",
        "    while sample_count < num_samples:\n",
        "        samples = model.p_sample(batch_size).cpu().detach().numpy().tolist()\n",
        "\n",
        "        for sample in samples:\n",
        "            if sample in train_data:\n",
        "                train_count += 1\n",
        "            elif sample in val_data:\n",
        "                val_count += 1\n",
        "            else:\n",
        "                other_count += 1\n",
        "            sample_count += 1\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "    return train_count, val_count, other_count\n",
        "\n",
        "def plot_evolution(epochs, examples, step_name='Step', filename='evolution.gif'):\n",
        "    fig = plt.figure()\n",
        "    im = plt.imshow(examples[0], interpolation='none')\n",
        "\n",
        "    def init():\n",
        "        fig.suptitle(step_name + ': 0')\n",
        "        im.set_data(examples[0])\n",
        "        return [im]\n",
        "\n",
        "    def animate(i):\n",
        "        fig.suptitle(step_name + ': {}'.format(epochs[i]))\n",
        "        im.set_array(examples[i])\n",
        "        return [im]\n",
        "\n",
        "    # generate the animation\n",
        "    ani = FuncAnimation(fig, animate, init_func=init,\n",
        "                        frames=len(examples), interval=300, repeat=True) \n",
        "    \n",
        "    ani.save( filename, writer='imagemagick', fps=2)\n",
        "\n",
        "    fig.clf()"
      ],
      "metadata": {
        "id": "8M0IPkvgDxcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "whPBcGL8WVjC",
        "outputId": "2cbf7edf-c277-446b-f378-ab74992b7e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0: Loss: 126772.9765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 1: Loss: 126681.5859375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 2: Loss: 126618.96875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 3: Loss: 126550.9921875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 4: Loss: 126484.3984375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 5: Loss: 126420.9296875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 6: Loss: 126351.1328125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 7: Loss: 126283.8828125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 8: Loss: 126210.8359375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 9: Loss: 126144.0859375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 10: Loss: 126065.7734375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 11: Loss: 125996.4765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 12: Loss: 125923.625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 13: Loss: 125840.7421875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 14: Loss: 125762.828125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 15: Loss: 125675.328125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 16: Loss: 125596.46875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 17: Loss: 125493.6796875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 18: Loss: 125387.46875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 19: Loss: 125275.8828125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 20: Loss: 125158.9765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 21: Loss: 125024.4453125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 22: Loss: 124894.890625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 23: Loss: 124728.6015625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 24: Loss: 124571.6484375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 25: Loss: 124388.03125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 26: Loss: 124229.109375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 27: Loss: 123997.1796875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 28: Loss: 123826.9765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 29: Loss: 123570.7109375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 30: Loss: 123376.015625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 31: Loss: 123104.8359375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 32: Loss: 122881.8984375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 33: Loss: 122578.8203125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 34: Loss: 122335.453125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 35: Loss: 122015.53125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 36: Loss: 121731.765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 37: Loss: 121374.5390625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 38: Loss: 121116.1171875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 39: Loss: 120711.6328125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 40: Loss: 120409.828125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 41: Loss: 120021.6953125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 42: Loss: 119692.125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 43: Loss: 119257.40625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 44: Loss: 118889.4453125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 45: Loss: 118468.1171875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 46: Loss: 118124.1015625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 47: Loss: 117638.9296875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 48: Loss: 117257.078125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 49: Loss: 116813.96875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 50: Loss: 116386.1640625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 51: Loss: 115922.796875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 52: Loss: 115527.53125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 53: Loss: 115002.4921875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 54: Loss: 114586.375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 55: Loss: 114145.0546875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 56: Loss: 113753.1640625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 57: Loss: 113276.625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 58: Loss: 112891.75 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 59: Loss: 112401.21875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 60: Loss: 112062.3359375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 61: Loss: 111613.7734375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 62: Loss: 111266.96875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 63: Loss: 110832.7109375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 64: Loss: 110541.765625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 65: Loss: 110047.5234375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 66: Loss: 109800.203125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 67: Loss: 109363.4140625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 68: Loss: 109131.40625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 69: Loss: 108769.84375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 70: Loss: 108476.6328125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 71: Loss: 108149.2890625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 72: Loss: 107897.0234375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 73: Loss: 107609.578125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 74: Loss: 107410.609375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 75: Loss: 107090.890625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 76: Loss: 106969.671875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 77: Loss: 106676.75 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 78: Loss: 106544.3984375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 79: Loss: 106294.8671875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 80: Loss: 106191.734375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 81: Loss: 105956.3984375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 82: Loss: 105889.890625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 83: Loss: 105587.2265625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 84: Loss: 105505.484375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 85: Loss: 105295.578125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 86: Loss: 105215.4453125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 87: Loss: 105007.6015625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 88: Loss: 104819.78125 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 89: Loss: 104782.5859375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 90: Loss: 104657.859375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 91: Loss: 104424.8046875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 92: Loss: 104449.3984375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 93: Loss: 104250.2265625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 94: Loss: 104170.3359375 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 95: Loss: 103977.2890625 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 96: Loss: 103953.421875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 97: Loss: 103813.46875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 98: Loss: 103756.796875 Train Proportion: 0.0 Val Proportion: 0.0\n",
            "Epoch: 99: Loss: 103596.09375 Train Proportion: 0.0 Val Proportion: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "T =  2000\n",
        "num_sample_steps = 30\n",
        "epochs = 100\n",
        "lr = 0.5\n",
        "save_every_n_epochs = 10\n",
        "num_examples = 10\n",
        "num_val_samples = 1024\n",
        "val_batch_size = 64\n",
        "clip_thresh = 1.0\n",
        "data_batch_size = 64\n",
        "\n",
        "\n",
        "cur_ind = 0\n",
        "\n",
        "\n",
        "data = []\n",
        "with open(os.path.join('train.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(torch.FloatTensor([float(x) for x in line.strip()]).to(\"cpu\"))\n",
        "\n",
        "data = torch.vstack(data)\n",
        "num_data = data.size()[0]\n",
        "sequence_length = data.size()[1]\n",
        "index_permutation = torch.randperm(num_data)\n",
        "\n",
        "# def get_next():\n",
        "#     global index_permutation\n",
        "#     global cur_ind\n",
        "#     global data_batch_size\n",
        "#     global data\n",
        "    \n",
        "#     if cur_ind < num_data:\n",
        "#         indices = index_permutation[cur_ind : cur_ind + data_batch_size]\n",
        "#         cur_ind += data_batch_size\n",
        "#         return data[indices]\n",
        "\n",
        "#     cur_ind = 0\n",
        "#     return None\n",
        "\n",
        "\n",
        "# def calculate_beta_tilde_t(beta_tilde_t, cur_t):\n",
        "#     a = beta_tilde_t[cur_t-1] + beta_t(cur_t)\n",
        "#     b = beta_t(cur_t) * beta_tilde_t[cur_t-1]\n",
        "#     return (a - b)\n",
        "\n",
        "# def validate(model, num_samples, batch_size):\n",
        "#     train_data = []\n",
        "#     with open(os.path.join('train.txt'), 'r') as f:\n",
        "#         for line in f:\n",
        "#             train_data.append(torch.FloatTensor([float(x) for x in line.strip()]).to(\"cpu\"))\n",
        "#         train_data = torch.vstack(train_data)\n",
        "\n",
        "#     train_data = train_data.cpu().detach().numpy().tolist()\n",
        "\n",
        "#     val_data = []\n",
        "#     with open(os.path.join('val.txt'), 'r') as f:\n",
        "#         for line in f:\n",
        "#             val_data.append(torch.FloatTensor([float(x) for x in line.strip()]).to(\"cpu\"))\n",
        "#         val_data = torch.vstack(val_data)\n",
        "\n",
        "#     val_data = val_data.cpu().detach().numpy().tolist()\n",
        "\n",
        "#     train_count = 0\n",
        "#     val_count = 0\n",
        "#     other_count = 0\n",
        "\n",
        "#     sample_count = 0\n",
        "#     while sample_count < num_samples:\n",
        "#         samples = model.p_sample(batch_size).cpu().detach().numpy().tolist()\n",
        "\n",
        "#         for sample in samples:\n",
        "#             if sample in train_data:\n",
        "#                 train_count += 1\n",
        "#             elif sample in val_data:\n",
        "#                 val_count += 1\n",
        "#             else:\n",
        "#                 other_count += 1\n",
        "#             sample_count += 1\n",
        "#             if sample_count >= num_samples:\n",
        "#                 break\n",
        "\n",
        "#     return train_count, val_count, other_count\n",
        "\n",
        "# def plot_evolution(epochs, examples, step_name='Step', filename='evolution.gif'):\n",
        "#     '''Given a sequence of batches of samples, this animates their evolution.\n",
        "#     Useful for showing how a particular sample changes during training.\n",
        "#     Also useful for illustrating the reverse process.'''\n",
        "#     fig = plt.figure()\n",
        "#     im = plt.imshow(examples[0], interpolation='none')\n",
        "\n",
        "#     def init():\n",
        "#         fig.suptitle(step_name + ': 0')\n",
        "#         im.set_data(examples[0])\n",
        "#         return [im]\n",
        "\n",
        "#     def animate(i):\n",
        "#         fig.suptitle(step_name + ': {}'.format(epochs[i]))\n",
        "#         im.set_array(examples[i])\n",
        "#         return [im]\n",
        "\n",
        "#     # generate the animation\n",
        "#     ani = FuncAnimation(fig, animate, init_func=init,\n",
        "#                         frames=len(examples), interval=300, repeat=True) \n",
        "    \n",
        "#     ani.save( filename, writer='imagemagick', fps=2)\n",
        "\n",
        "#     fig.clf()\n",
        "\n",
        "\n",
        "diffusion_model = DiffusionModel(sequence_length, num_sample_steps, T)\n",
        "optimizer = optim.SGD(diffusion_model.parameters(), lr=lr)\n",
        "\n",
        "diffusion_model.zero_grad()\n",
        "\n",
        "m = Binomial(1, torch.zeros((data_batch_size, sequence_length)).fill_(0.5))\n",
        "random_data = m.sample().to(\"cpu\")\n",
        "\n",
        "out = diffusion_model(random_data)\n",
        "out.backward()\n",
        "\n",
        "\n",
        "losses = []\n",
        "examples_per_epoch = []\n",
        "proportions = {'train': [], 'val':[], 'other':[]}\n",
        "random_seed = Binomial(1, torch.zeros((num_examples, sequence_length)).fill_(0.5)).sample().to(\"cpu\")\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    avg_loss = 0.0\n",
        "    batch_count = 0\n",
        "    batch = get_next()\n",
        "    while batch is not None:\n",
        "        optimizer.zero_grad()\n",
        "        output = diffusion_model(batch)\n",
        "        if torch.isnan(output):\n",
        "            raise Exception('learning rate  is too high')\n",
        "        output.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(diffusion_model.parameters(), clip_thresh)\n",
        "        optimizer.step()\n",
        "        batch = get_next()\n",
        "        batch_count += 1\n",
        "        avg_loss += output.detach()\n",
        "    avg_loss = avg_loss / batch_count\n",
        "    losses.append(avg_loss.item())\n",
        "\n",
        "    sample_data = diffusion_model.p_sample(num_examples, random_seed)\n",
        "    examples_per_epoch.append(sample_data)\n",
        "\n",
        "    train_count, val_count, other_count = validate(diffusion_model,num_val_samples,val_batch_size)\n",
        "\n",
        "    proportions['train'].append(train_count/num_val_samples)\n",
        "    proportions['val'].append(val_count/num_val_samples)\n",
        "    proportions['other'].append(other_count/num_val_samples)\n",
        "\n",
        "    print('Epoch: {}: Loss: {} Train Proportion: {} Val Proportion: {}'.format(epoch,losses[-1],proportions['train'][-1],proportions['val'][-1]))\n",
        "\n",
        "results = {'losses': losses,\n",
        "            'examples_per_epoch': [x.tolist() for x in examples_per_epoch],\n",
        "            'proportions': proportions}\n",
        "with open('results.json', 'w') as fp:\n",
        "    json.dump(results, fp)\n",
        "\n",
        "epochs_plotted = [x for x in range(0, epochs)]\n",
        "\n",
        "plt.plot(epochs_plotted, losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig(f\"loss_curve_{lr}_{epochs}_SGD.png\")\n",
        "plt.clf()\n",
        "\n",
        "numpy_examples = [x.cpu().detach().numpy() for x in examples_per_epoch]\n",
        "plot_evolution(epochs_plotted,\n",
        "            numpy_examples,\n",
        "            step_name = 'Epoch',\n",
        "            filename=f'sample_evolution_throughout_training_{lr}_{epochs}_SGD.gif')\n",
        "\n",
        "\n",
        "\n",
        "for key, item in proportions.items():\n",
        "    plt.plot(epochs_plotted, item, label=key)\n",
        "plt.title('Validation Proportions')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Proportion')\n",
        "plt.legend()\n",
        "plt.savefig(f'validation_{lr}_{epochs}_SGD.png')\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Binomial\n",
        "from math import comb, pow, log2\n",
        "\n",
        "\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, sequence_length, num_sample_steps, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = Model(sequence_length, T).to(\"cpu\")\n",
        "        \n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_sample_steps = num_sample_steps\n",
        "\n",
        "        self.final_noise = None\n",
        "        self.T = T\n",
        "        self.beta_tilde_t = [torch.zeros((self.sequence_length)).to(\"cpu\")]\n",
        "        for cur_t in range(1, self.T+1):\n",
        "            self.beta_tilde_t.append((self.beta_tilde_t[cur_t-1] + self.beta_t(cur_t) - (self.beta_t(cur_t)*self.beta_tilde_t[cur_t-1])))\n",
        "        self.beta_tilde_t = torch.stack(self.beta_tilde_t)\n",
        "\n",
        "        self.H_start = self.entropy_of_q_conditional(self.sequence_length, self.beta_tilde_t[1,0].item())\n",
        "        self.H_end = self.entropy_of_q_conditional(self.sequence_length, self.beta_tilde_t[self.T, 0].item())\n",
        "        self.H_prior = self.entropy_of_prior(self.sequence_length)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def entropy_of_q_conditional(self, sequence_length, beta_tilde_t):\n",
        "        total_entropy = 0.0\n",
        "        eps = 1e-30\n",
        "        for k in range(0, sequence_length+1):\n",
        "            n_c_k = comb(sequence_length, k)\n",
        "            prob = pow((1.0-(0.5*beta_tilde_t)), k) * pow(0.5*beta_tilde_t, sequence_length-k)\n",
        "            cur_entropy = n_c_k * prob * log2(prob + eps)\n",
        "            total_entropy += cur_entropy\n",
        "        return -1.0 * total_entropy\n",
        "\n",
        "    def entropy_of_prior(self, sequence_length):\n",
        "        '''Assuming all Bernoulli distributions in prior have prob 0.5.\n",
        "        Fun fact: this basically just returns float(sequence_length)'''\n",
        "        eps = 1e-30\n",
        "        total_entropy = 0.0\n",
        "        for k in range(0, sequence_length+1):\n",
        "            n_choose_k = comb(sequence_length, k)\n",
        "            prob = pow((1.0-0.5), k) * pow(0.5, sequence_length-k)\n",
        "            cur_entropy = n_choose_k * prob * log2(prob + eps)\n",
        "            total_entropy += cur_entropy\n",
        "        return -1.0 * total_entropy\n",
        "\n",
        "    def p_conditional_prob(self, x_t, t):\n",
        "        return self.model(x_t, t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_step(self, x, t):\n",
        "        return torch.bernoulli(self.model(x, t))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, batch_size, x=None):\n",
        "        if x is None:\n",
        "            init_prob = torch.empty((batch_size, self.sequence_length)).fill_(0.5).to(\"cpu\")\n",
        "            x = torch.bernoulli(init_prob)\n",
        "        else:\n",
        "            assert batch_size == x.size(dim=0)\n",
        "\n",
        "        for cur_t in range(self.T, 0, -1):\n",
        "            x = torch.bernoulli(self.p_conditional_prob(x, cur_t))\n",
        "        return x\n",
        "\n",
        "    def beta_t(self, t):\n",
        "        return 1.0/(self.T-t+1)\n",
        "\n",
        "    def kl_div(self, q, p):\n",
        "        eps = 1e-30\n",
        "        '''KL Divergence of two multivariate Bernoulli distributions'''\n",
        "        q = torch.clip(q, min=1e-10, max=1-(1e-7))\n",
        "        p = torch.clip(p, min=1e-10, max=1-(1e-7))\n",
        "        return torch.sum((q * torch.log2((q/p) + eps)) + ((1.0-q) * torch.log2(((1.0-q)/(1.0-p)) + eps)), dim=1)\n",
        "\n",
        "\n",
        "    def q_conditional_prob(self, x_t, t):\n",
        "        # had to change the beta_t equation here to keep indexing consistent\n",
        "        return (x_t * (1.0 - self.beta_t(t+1))) + 0.5 * self.beta_t(t+1)\n",
        "\n",
        "    def q_conditional_prob_wrt_x_0(self, x_0, t):\n",
        "        beta_tilde_t = self.beta_tilde_t[t].expand(x_0.size())\n",
        "        return ((x_0 * (1.0 - beta_tilde_t)) + 0.5 * beta_tilde_t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def q_step(self, x, t):\n",
        "        probs = self.q_conditional_prob(x, t)\n",
        "        return torch.bernoulli(probs)\n",
        "\n",
        "    def q_sample(self, x_0, t):\n",
        "        return torch.bernoulli(self.q_conditional_prob_wrt_x_0(x_0, t))\n",
        "    \n",
        "    def forward(self, x_0):\n",
        "        # the monte carlo sampling is performed using the minibatch\n",
        "        total_loss = torch.zeros((x_0.size(dim=0),)).to(\"cpu\")\n",
        "        for t in range(1, self.T + 1):\n",
        "            x_t = self.q_sample(x_0, t)\n",
        "            beta_t = self.beta_t(t)\n",
        "            posterior = x_0*(1-self.beta_tilde_t[t-1]) + 0.5*self.beta_tilde_t[t-1]\n",
        "            posterior *= x_t * (1-0.5*beta_t) + (1 - x_t) * (0.5*beta_t)\n",
        "            normalizing_constant = x_t * self.q_conditional_prob_wrt_x_0(x_0, t) + (1-x_t) * (1-self.q_conditional_prob_wrt_x_0(x_0, t))\n",
        "            posterior = posterior / normalizing_constant\n",
        "            kl_divergence = self.kl_div(posterior,self.p_conditional_prob(x_t, t))\n",
        "\n",
        "            total_loss += kl_divergence + self.H_start - self.H_end + self.H_prior\n",
        "\n",
        "\n",
        "            if(t == self.T):\n",
        "                self.final_noise = x_t\n",
        "\n",
        "        return torch.mean(total_loss)\n"
      ],
      "metadata": {
        "id": "9WQW4nroWobk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, sequence_length, T):\n",
        "        super().__init__()\n",
        "        print(sequence_length)\n",
        "        self.shared_layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(sequence_length, 70),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(70, 70),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.output_layers = nn.ModuleList([torch.nn.Linear(70, sequence_length) for x in range(T)])\n",
        "        self.outputs_sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.shared_layers(x)\n",
        "        x = self.output_layers[t-1](x)\n",
        "        x = self.outputs_sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lYUA9qoPEyJ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}